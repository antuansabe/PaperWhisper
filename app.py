"""
PaperWhisper - Interfaz Streamlit
Aplicaci√≥n para conversar con documentos PDF usando RAG + Mistral AI
"""

import os
from typing import List, Tuple, Optional

import streamlit as st
from dotenv import load_dotenv
from langchain_mistralai import ChatMistralAI

from src.rag_engine import (
    ingest_pdf_to_index,
    similarity_search,
    read_pdf,
    INDEX_PATH,
    DEFAULT_MODEL_NAME
)

# Cargar variables de entorno
load_dotenv()

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="PaperWhisper",
    page_icon="üìÑ",
    layout="wide",
    initial_sidebar_state="expanded"
)


@st.cache_resource(show_spinner=False)
def get_mistral_llm(model: str = "mistral-small-latest") -> Optional[ChatMistralAI]:
    """
    Inicializa el modelo de Mistral AI.
    Se cachea para evitar reinicializaciones.

    Args:
        model: Nombre del modelo de Mistral (mistral-small-latest, mistral-medium, etc.)

    Returns:
        Instancia de ChatMistralAI o None si no hay API key
    """
    api_key = os.getenv("MISTRAL_API_KEY")
    if not api_key:
        st.warning("‚ö†Ô∏è MISTRAL_API_KEY no configurada. Solo se habilitar√° b√∫squeda sem√°ntica.")
        return None

    try:
        return ChatMistralAI(
            model=model,
            temperature=0,  # Respuestas determin√≠sticas
            max_tokens=1024,  # L√≠mite de tokens en respuesta
        )
    except Exception as e:
        st.error(f"‚ùå Error inicializando Mistral: {e}")
        return None


def generate_answer_with_mistral(
    llm: ChatMistralAI,
    query: str,
    context_chunks: List[Tuple[str, float]]
) -> str:
    """
    Genera una respuesta usando Mistral AI con el contexto recuperado.

    Args:
        llm: Instancia de ChatMistralAI
        query: Pregunta del usuario
        context_chunks: Lista de (chunk_text, score) del RAG

    Returns:
        Respuesta generada por el LLM
    """
    # Construir contexto a partir de los chunks
    context = "\n\n---\n\n".join([chunk for chunk, _ in context_chunks])

    # Prompt optimizado para RAG
    system_prompt = """Eres un asistente experto que responde preguntas bas√°ndote √öNICAMENTE en el contexto proporcionado.

Reglas importantes:
1. Responde SOLO con informaci√≥n presente en el contexto
2. Si la respuesta no est√° en el contexto, di: "No encuentro esa informaci√≥n en el documento"
3. S√© preciso y conciso
4. Cita fragmentos relevantes cuando sea √∫til
5. Si hay informaci√≥n parcial, ind√≠calo claramente"""

    user_prompt = f"""Contexto del documento:
{context}

Pregunta del usuario:
{query}

Respuesta:"""

    # Invocar Mistral
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        response = llm.invoke(messages)
        return response.content
    except Exception as e:
        return f"‚ùå Error generando respuesta: {e}"


def main():
    """Funci√≥n principal de la aplicaci√≥n Streamlit"""

    # Header con imagen adaptable al tema
    st.markdown("""
        <style>
        /* Mostrar logo oscuro por defecto (modo d√≠a) */
        .logo-light {
            display: block;
        }
        .logo-dark {
            display: none;
        }

        /* En modo oscuro, invertir */
        @media (prefers-color-scheme: dark) {
            .logo-light {
                display: none;
            }
            .logo-dark {
                display: block;
            }
        }
        </style>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        # Logo para modo d√≠a
        st.markdown('<div class="logo-light">', unsafe_allow_html=True)
        st.image("ppaper-dia.png", use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)

        # Logo para modo noche
        st.markdown('<div class="logo-dark">', unsafe_allow_html=True)
        st.image("ppaper.png", use_container_width=True)
        st.markdown('</div>', unsafe_allow_html=True)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("---")

    # Sidebar - Configuraci√≥n
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Configuraci√≥n")
        st.markdown("")

        # Secci√≥n de modelos
        with st.expander("ü§ñ Modelos de IA", expanded=True):
            # Modelo de Mistral
            mistral_model = st.selectbox(
                "Modelo de IA",
                options=[
                    "mistral-small-latest",
                    "mistral-medium-latest",
                    "mistral-large-latest",
                    "open-mistral-7b",
                    "open-mixtral-8x7b"
                ],
                index=0,
                help="Modelo de Mistral AI a usar para generar respuestas"
            )

            # Modelo de embeddings (oculto por defecto)
            embeddings_model = os.getenv("EMBEDDINGS_MODEL", DEFAULT_MODEL_NAME)

        # Secci√≥n de par√°metros
        with st.expander("üéõÔ∏è Par√°metros", expanded=True):
            # Top-K chunks
            top_k = st.slider(
                "Fragmentos a recuperar",
                min_value=1,
                max_value=10,
                value=4,
                help="N√∫mero de fragmentos relevantes del documento"
            )

        st.markdown("---")

        # Bot√≥n para limpiar √≠ndice
        if st.button("üóëÔ∏è Limpiar √≠ndice FAISS", help="Elimina el √≠ndice guardado y fuerza reconstrucci√≥n"):
            if os.path.exists(INDEX_PATH):
                try:
                    import shutil
                    if os.path.isdir(INDEX_PATH):
                        shutil.rmtree(INDEX_PATH)
                    st.success("‚úÖ √çndice eliminado")
                    st.rerun()
                except Exception as e:
                    st.error(f"‚ùå Error eliminando √≠ndice: {e}")
            else:
                st.info("‚ÑπÔ∏è No hay √≠ndice para eliminar")

        st.markdown("---")

        # Estado del sistema con mejor dise√±o
        st.markdown("### üìä Estado del Sistema")

        # API Key status
        api_key = os.getenv("MISTRAL_API_KEY")
        if api_key and api_key != "your_mistral_api_key_here":
            st.markdown("üü¢ **IA:** Conectada")
        else:
            st.markdown("üî¥ **IA:** Desconectada")
            st.caption("Configura `MISTRAL_API_KEY` en `.env`")

        # √çndice status
        if os.path.exists(INDEX_PATH):
            st.markdown("üü¢ **√çndice:** Listo")
        else:
            st.markdown("‚ö™ **√çndice:** Sin crear")

        st.markdown("")

        # Info adicional
        with st.expander("‚ÑπÔ∏è Acerca de"):
            st.markdown("""
                **PaperWhisper** v1.0

                Conversa con tus documentos PDF usando:
                - ü§ñ Mistral AI
                - üîç FAISS
                - üß¨ HuggingFace

                [GitHub](https://github.com/antuansabe/PaperWhisper) ‚Ä¢ [Docs](./QUICKSTART.md)
            """)

    # Main content con dise√±o mejorado
    st.markdown("### üì§ Paso 1: Sube tu documento")

    uploaded_file = st.file_uploader(
        "Arrastra un PDF aqu√≠ o haz clic para seleccionar",
        type=["pdf"],
        help="El PDF se procesar√° autom√°ticamente para b√∫squedas",
        label_visibility="collapsed"
    )

    # Procesar PDF si se sube
    db = None
    if uploaded_file is not None:
        # Guardar PDF temporalmente
        pdf_path = os.path.join("data", uploaded_file.name)
        os.makedirs("data", exist_ok=True)

        with open(pdf_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # Ingerir PDF al √≠ndice con mejor feedback
        with st.spinner("üîÑ Procesando tu documento..."):
            try:
                db = ingest_pdf_to_index(pdf_path, model_name=embeddings_model)
                st.success(f"‚úÖ **{uploaded_file.name}** listo para consultas")
            except Exception as e:
                st.error(f"‚ùå Error procesando PDF: {e}")
                return

        # Vista previa del documento en un expander
        with st.expander("üëÅÔ∏è Ver vista previa del documento", expanded=False):
            try:
                preview_text = read_pdf(pdf_path)[:1500]
                st.text_area(
                    "Primeros 1500 caracteres",
                    value=preview_text,
                    height=250,
                    disabled=True,
                    label_visibility="collapsed"
                )
                total_chars = len(read_pdf(pdf_path))
                st.caption(f"üìä Documento completo: {total_chars:,} caracteres")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è No se pudo generar vista previa: {e}")

        st.markdown("---")

    # Secci√≥n de consulta con mejor dise√±o
    st.markdown("### üí¨ Paso 2: Haz tu pregunta")

    query = st.text_input(
        "Pregunta",
        placeholder="Ej: ¬øCu√°l es el tema principal del documento?",
        help="Pregunta en lenguaje natural sobre el contenido del PDF",
        label_visibility="collapsed"
    )

    # Inicializar LLM
    llm = get_mistral_llm(mistral_model)

    # Bot√≥n de consulta
    if st.button("üîç Preguntar", type="primary", use_container_width=True):
        if db is None:
            st.error("‚ùå Primero debes subir un PDF para poder hacer preguntas")
            return

        if not query.strip():
            st.warning("‚ö†Ô∏è Por favor escribe una pregunta")
            return

        # B√∫squeda de chunks relevantes
        with st.spinner("üîç Buscando informaci√≥n relevante en el documento..."):
            try:
                results: List[Tuple[str, float]] = similarity_search(db, query, k=top_k)
            except Exception as e:
                st.error(f"‚ùå Error en b√∫squeda sem√°ntica: {e}")
                return

        # Generar respuesta con Mistral primero (si est√° disponible)
        if llm is None:
            st.warning("‚ö†Ô∏è Configura `MISTRAL_API_KEY` en `.env` para habilitar respuestas con IA")
            st.info("üìö Mostrando solo b√∫squeda sem√°ntica:")
        else:
            st.markdown("### ü§ñ Respuesta")

            with st.spinner("ü§ñ Generando respuesta..."):
                try:
                    answer = generate_answer_with_mistral(llm, query, results)

                    # Mostrar respuesta en un contenedor destacado
                    st.markdown(f"""
                        <div style='background-color: #f0f2f6; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #FF4B4B;'>
                            {answer}
                        </div>
                    """, unsafe_allow_html=True)
                except Exception as e:
                    st.error(f"‚ùå Error generando respuesta: {e}")

        # Mostrar contexto recuperado en expander
        st.markdown("")
        with st.expander(f"üìö Ver fragmentos relevantes ({len(results)} encontrados)", expanded=False):
            st.caption("Los fragmentos m√°s similares a tu pregunta:")
            for i, (chunk, score) in enumerate(results, start=1):
                similarity_pct = (1 - score) * 100  # Convertir distancia a porcentaje
                st.markdown(f"**Fragmento {i}** ‚Äî Relevancia: {similarity_pct:.1f}%")
                st.text(chunk)
                st.markdown("---")

    # Footer mejorado
    st.markdown("<br><br>", unsafe_allow_html=True)
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; padding: 1rem 0;'>
            <p style='color: #666; font-size: 0.9rem;'>
                Hecho con ‚ù§Ô∏è usando <strong>Mistral AI</strong>, <strong>LangChain</strong> y <strong>FAISS</strong>
            </p>
            <p style='color: #999; font-size: 0.8rem; margin-top: 0.5rem;'>
                <a href='https://github.com/antuansabe/PaperWhisper' target='_blank' style='color: #FF4B4B; text-decoration: none;'>
                    ‚≠ê GitHub
                </a> ‚Ä¢
                <a href='./QUICKSTART.md' style='color: #FF4B4B; text-decoration: none;'>
                    üìñ Docs
                </a>
            </p>
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
